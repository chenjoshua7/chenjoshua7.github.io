<!DOCTYPE html>
<html lang="en">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-W57V1PRPVC"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-W57V1PRPVC');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <title>Curse of Dimensionality</title>
    <link rel="stylesheet" href="../template.css">
    <style>
        .image {
            color: white;
            text-align: center;
        }

        .image img{
            width: 50%;
            height: auto;
            border-radius: 10px; 
        }

        .rounded-image {
            padding: 9px;
            border-radius: 10px; /* Adding a border to visually see the effect of border-radius */
        }

        .visual {
            color: white;
            text-align: center;
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .visual img{
            border-radius: 10px;
            
        }

        .graph {
            text-align: center;
            align-items: center;
            justify-content: center;
            margin: 0 0px;
        }
        
        .graph img {
            border-radius: 10px; 
            width: 90%;
            height: auto;
        }

        .line {
            margin: 0px 200px;
            height: 10px; 
            background-color: #ff0000; 
            position: relative;
        }

        .marker {
            height: 100%;
            background-color: #1900ff;
            position: absolute;
        }

        .start {
            left: 10%; /* alpha = 0.1 */
            width: 80%; /* from alpha to 1 - alpha */
        }

        .end {
            right: 10%; /* 1 - alpha = 0.9 */
            width: 10%; /* width of the boundary area */
        }

        .label {
            position: absolute;
            top: 30px; /* position below the line */
            font-size: 14px;
            color: #ffffff;
        }
        .alpha {
            left: 10%; /* align with alpha */
        }
        
        .one-minus-alpha {
            left: 90%; /* align with 1 - alpha */
        }

        table {
            font-family: Arial, sans-serif;
            border-collapse: collapse;
            width: 50%;
            margin-left: auto;  /* Centers table */
            margin-right: auto; /* Centers table */
        }

          
          td, th {
            border: 1px solid #dddddd;
            text-align: left;
            padding: 10px;
          }
          
          tr:nth-child(1) {
            background-color: hsla(0, 0%, 0%, 0.602);
          }

          tr:nth-child(even) {
            background-color: hsla(256, 100%, 94%, 0.159);
          }

        .outer-square {
            margin: 0px 200px;
            width: 550px;
            height: 550px; 
            background-color: #ff0000; 
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
            border: 1px solid black;
        }
        .inner-square {
            width: 80%; /* 100% - 2*20% if alpha is 0.2 */
            height: 80%; /* Same as width */
            background-color: #3c00ff; /* Green color for visibility */
            border: 1px solid black;
        }
        .label1 {
            position: absolute;
            font-size: 14px;
            color: #ffffff;
        }
        .label-alpha1 {
            bottom: -20px;
            left: 10%;
        }
        .label-one-minus-alpha1 {
            bottom: -20px;
            right: 10%;
        }


    </style>
</head>
<body>
    <div class="banner">
        <a href= "#"> </a>
        <a href= "#"> </a>
        <a href="/index.html">Home</a>
        <a href="/resume.html">Resume</a>
        <div class="dropdown">
            <a href="/math.html"><button class="dropbtn">Math</button></a>
            <div class="dropdown-content">
                <a href="/math/perceptron.html">Perceptron</a>
                <a href="/math/pandemic.html">Pandemic Modeling</a>
                <a href="/math/cod.html">Curse of Dimensionality</a>
            </div>
        </div>
    
        <div class="dropdown">
            <a href="/projects.html"><button class="dropbtn">Projects</button></a>
            <div class="dropdown-content">
                <a href="/projects/recommendation.html">MovieLens Recommendation</a>
                <a href="/projects/nlp.html">BERT Classification</a>
                <a href="/projects/gspmulticlass.html">Urban Land Covers Class-<br/>ification with Linear Models</a>
                <a href="/projects/basketball.html">Lakers 23-24 Analysis</a>
            </div>
        </div>
        <a href="/other.html">Other Stuff</a>
        <a href= "#"> </a>
        <a href= "#"> </a>
    </div>

    <div class="backgd">
        <div class="textblock">
            <h1 class="center-text">The Curse of Dimensionality</h1>
            <p> This page was inspired by a conversation I was having with my roommate.
                The <i>Curse of Dimensionality</i> is something we have discussed repeatedly in our classes
                but I wanted to do some exploration to get a deeper understanding of it and its affect
                with certain algorithms.
            </p>
            <hr>
            <h2 class="center-text">Explaining of the Curse of Dimensionality</h2>
            <p>The core of Machine Learning is to uncover the true posterior distribution of a hyperdimensional 
                space given the data, aiming to understand the probability distribution of what is being predicted 
                based on the observed inputs. Very simply if we understand how the output relates to the data, we can use
                the data to predict the output.
            </p>
            <p>
                However, understanding hyperdimensional space is not easy and increasing dimensions is 
                makes problems exponentially difficult.
            </p>
            <p>
                Let's take a simple example. In a one dimensional space (a line) of length 1, the probability that
                a random point is on the inside of the line (with boundaries &alpha;) is \(1-2 \cdot &alpha;\)
            </p>
            <br/>

            <figcaption class="center-text">1 Dimension with Boundary α</figcaption>
            <br/>
            <div class="line">
                <div class="marker start"></div>
                <div class="label alpha">α</div>
                <div class="label one-minus-alpha">1-α</div>
            </div>
            <br/>
            <br/>
            <br/>
            <p> With 2 dimensions, this probability changes to \(\left(1-2 \cdot α\right)^2\)</p>
            <br/>
            <figcaption class="center-text">2 Dimension with Boundary α</figcaption>
            <br/>
            <div class="outer-square">
                <div class="inner-square"></div>
                <div class="label1 label-alpha1">α</div>
                <div class="label1 label-one-minus-alpha1">1-α</div>
            </div>
            <br/>
            <p>Following this, we have the probability that in <i>d</i>-dimensional space, the 
            probability the probability of closeness with boundary α is \(\left( 1-2\cdot α\right)^d\) - which rapidly
            decreases as d grows.</p>

            <p>In 100 dimensional space, with a boundary only 0.01, the probability that a point is in the middle is <i><strong>13.3%</strong></i>. Let's 
            put it another way. In you randomly place a point in a 100 dimension box, 86.7% of the points will be in the 1 percent edge of the box.</p>

            <p>When trying to work in high dimensional space, distances clearly become very, for lack a better word, funky. Below, I demonstrate this
                concept using some common algorithms.
            </p>
            <p>There are many interesting articles and papers explaining and exploring the Curse of Dimensionality. One 
                place to start is <a href="https://towardsdatascience.com/curse-of-dimensionality-a-curse-to-machine-learning-c122ee33bfeb"> here.</a>
            </p>
            <br/>
            <hr>
            <br/>
            <h2 class="center-text">Demonstration with kNN</h2>
            <p> k-Neigherst Neighbors is an algorithm that suffers greatly from high dimensionality as
                it is based purely on relational distance. Therefore, it is an ideal alogrithm to visual
                how dimensionality warps distance.
            </p>
            <h3 class="center-text"> Generating the Data:</h3>

            <p> To test this I generated the data in the following way. We generate data for 3 classes based on two dimensions.
                All of these were pulled from a bivariate gaussian with the identity matrix as the covariance matrix.
                <ol>
                    <h4>Cluster Centers (Distribution Means)</h4>
                    <li>Class 1 (Red): \(\left(-1.5, 1.5\right)\)</li>
                    <li>Class 2 (Blue): \(\left(1.5, 1.5\right)\)</li>
                    <li>Class 3 (Green): \( \left(0, -1.5 \cdot \left(1 - \sqrt{3}\right)\right) \)</li>
                </ol>
                It is important that cluster centers are equidistant from each other. Black dots below represent cluster centers.
                Also... to my students who asked, when will I need the Pythagorean Theorem or to know special right triangles...
                just used it.<br> <b>So ha.</b>
            </p>
            <div class="visual">
                <div class="graph">
                    <figcaption class="center-text">Generated Clusters</figcaption>
                    <img src="/images/data_clusters.png">
                </div>
            </div>
            <p> To demonstrate the curse of dimensionality, I then generate features that is just random noise pulled from a random
                uniform (-1, 1).</p>

            <h4 class="center-text">Results</h4>
            <p>See the results. 50 dimensions were chose on a log scale between 10 and 10000.
                Each dimension was tested 25 times and the average risk is plotted below.
            </p>

            <div class="visual">
                <div class="graph">
                    <img src="/images/1nnrisk.png">
                </div>
            </div>

            <p> Perhaps a cure for the Curse of Dimensionality is more samples! While this is the
                case for linear regression, with kNN, because the features are truly random, this is not so.

                k-NN relies on the distance between data points to make predictions. If the features are random 
                and do not correlate with the target variable, the distance metric becomes meaningless. Essentially, 
                the distances do not provide any useful information about the similarities between instances in terms of the output variable.
            </p>
            <p>
                See below. I hold the number of random features constant and add increase the samples drastically to no change
                in risk.
            </p>

            <div class="visual">
                <div class="graph">
                    <img src="/images/samplesize_risk.png">
                </div>
            </div>

            <br/>
            <hr>
            <h3 class="center-text">Applying Dimensionality Reduction</h3>
            <p> The purpose of this section is to explore how dimensionality reduction work 
                in higher dimensions. Below we see visualizations with PCA and t-SNE across
                different dimensions.
            </p>
            <div class="visual">
                <img src="images/pca_visuals.png" width=500px class="visual img"  hspace="10">   
                <img src="images/tsne-visuals.png" width=500px class="visual img" hspace="10">
        <br/>
    </div>
    <br/>
    <p>Below, we observe the impact of increased dimensionality on different dimensionality 
        reduction techniques. While all models are affected by higher dimensions, PCA appears 
        to mitigate some of the noise more effectively compared to t-SNE. t-SNE may be less 
        effective in this context  because it focuses on preserving local probabilities derived 
        from distances rather than global structure.
    </p>
    <div class="visual">
        <img src="images/combined_risks.png" width=500px class="visual img"  hspace="10"> <br/>
        
    </div>
    <figcaption class="center-text">1NN with Euclidean Distance</figcaption>
    </div>
    <br/>
    </div>

    <div class=btm_banner>
        © Copyright 2024 All rights reserved. Designed and developed from scratch by Joshua Chen.
    </div>
</body>
</html>
