<!DOCTYPE html>
<html lang="en">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-W57V1PRPVC"></script>
    <script src="/navbar.js?v=1.0"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-W57V1PRPVC');

        function toggleSection(sectionId) {
            var section = document.getElementById(sectionId);
            if (section.style.display === "block") {
                section.style.display = "none";
            } else {
                section.style.display = "block";
            }
        }
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>Function Approximation Project</title>
    <link rel="stylesheet" href="../template.css">
    <style>
        /* General styles */
        body {
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }

        a {
            color: white;
            text-decoration: none;
        }

        .center-text {
            text-align: center;
        }

        .textblock {
            background-color: rgba(23, 23, 23, 0.705);
            color: white;
            padding: 20px;
            margin: 20px auto;
            box-shadow: 0 0 20px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            max-width: 1300px;
        }

        .table-container {
            overflow-x: auto;
        }

        table {
            font-family: Arial, sans-serif;
            border-collapse: collapse;
            width: 100%;
            max-width: 600px;
            margin: 20px auto;
        }

        .video-wrapper {
            display: flex;
            justify-content: center;
            gap: 20px;
            flex-wrap: wrap; /* Wrap video containers for smaller screens */
        }

        .video-container {
            padding-left: 20px;
            padding-right: 20px;
        }

        video {
            width: 100%;
            max-width: 600px; /* Adjusted for better mobile display */
            height: auto;
        }

        td, th {
            font-size: 15px;
            border: 1px solid #dddddd;
            max-width: 90px;
            text-align: left;
            padding: 6px;
        }

        tr:nth-child(1) {
            background-color: hsla(0, 0%, 0%, 0.602);
        }

        tr:nth-child(even) {
            background-color: hsla(256, 100%, 94%, 0.159);
        }

        .image {
            text-align: center;
            margin: 20px 0;
        }

        .image img {
            max-width: 100%;
            height: auto;
            border-radius: 10px;
        }

        .note {
            background-color: #f8f9fa;
            color: #333;
            box-shadow: 0px 0 15px rgba(0, 0, 0, 0.1);
            border-radius: 10px;
            padding: 20px;
            margin: 0px 0;
            text-align: justify;
        }

        .btm_banner {
            background: #333;
            color: white;
            padding: 10px;
            text-align: center;
            width: 100%;
            position: relative;
        }

        .toggle-btn {
            width: 100%;
            background-color: #2b5ca5;
            color: white;
            padding: 0px;
            font-size: 18px;
            border: none;
            cursor: pointer;
            text-align: left;
            margin-top: 10px;
        }
        
        .toggle-btn:hover {
            background-color: #1a3e7a;
        }
        
        .toggle-section {
            display: block;
            background-color: rgba(43, 43, 43, 0.484);
            padding-left: 15px;
            margin-top: 10px;
        }
        
        .toggle-section p {
            margin: 10px 0;
            padding: 10px
        }
        
        .toggle-section a {
            color: white;
            text-decoration: none;
        }
        
        .toggle-section a:hover {
            text-decoration: underline;
        }

        @media (max-width: 768px) {
            .video-container{
                padding: 0;
                max-width: 100%;
                flex-direction: column;
            }

            .textblock {
                padding: 10px;
                margin: 10px;
            }

            table {
                min-width: 50px;
                font-size: 14px;
                max-width: 100%; 
            }

            td, th {
                padding: 6px;
            }

            .image {
                max-width: 100%;
            }

            .video-wrapper {
                padding: 6px;
                max-width: 100%;
            }
        }

        @media (max-width: 480px) {
            .video {
                max-width: 100%;
            }

            .image {
                max-width: 100%;
            }

            .textblock {
                padding: 5px;
                margin: 5px;
            }

            .video-wrapper {
                padding: 4px;
                max-width: 100%;
            }

            table {
                font-size: 12px;
            }

            td, th {
                padding: 4px;
            }
        }

    </style>
</head>
<body>
    <div id="navbar-placeholder"></div>

    <script>
        // Load the navbar.html file and insert it into the div with id="navbar-placeholder"
        fetch('/navbar.html')
            .then(response => response.text())
            .then(data => {
                document.getElementById('navbar-placeholder').innerHTML = data;
            });
    </script>
    
    <div class="textblock">
        
        <h1 class="center-text">Function Approximation with Taylor Series, Polynomial Regression, and Neural Networks</h1>
        <h4 class="center-text">Project Highlights: Visualization of Taylor Series Approximations, using Linear Regression with Taylor Polynomials to approximate derivatives, visualizing a Neural Network - coded from scratch with Numpy - approximate functions</h4>
        
        <p>Function approximation is a fundamental concept in numerical analysis and machine learning, where the goal is to approximate a target function using a simpler or more easily computable form. This project explores three different methods for function approximation: Taylor Series, Linear Regression with Polynomial Terms, and a Neural Network implemented from scratch using Numpy.</p>

        <div>
            <h2>Table of Contents</h2>
            <ol>
                <li><a href="#taylor-series">Taylor Series</a></li>
                <li><a href="#lr">Linear Regression with Polynomial Terms</a></li>
                <li><a href="#neural-network">Neural Network (Numpy)</a></li>
            </ol>
        </div>
        
        <button class="toggle-btn" onclick="toggleSection('ts')">
            <h2 id="taylor-series" class="center-text">Taylor Series</h2>
        </button>
        <div id="ts" class="toggle-section"> 
            <p>The Taylor series is a representation of a function as an infinite sum of terms calculated from the values of its derivatives at a single point. This method is particularly effective for approximating smooth functions that are well-behaved near the point of expansion.</p>
            <div class="note">
                <strong>Mathematical Explanation:</strong><br>
                The Taylor series of a function \( f(x) \) about a point \( a \) is given by:
                \[
                f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots
                \]
                Here, \( f'(a) \) is the first derivative of \( f \) at \( a \), \( f''(a) \) is the second derivative, and so on. The series provides an approximation of \( f(x) \) around the point \( a \).
            </div>

            <div class="image">
                <h3>Approximating Cosine</h3>
                <div class="video-wrapper">
                    <div>
                        <figcaption>Centered around 0 (MacLaurin Series)</figcaption>
                        <video controls="controls" name="Video Name">
                            <source src="functionapprox/ts_cosine_0.mp4">
                        </video>
                    </div>
                    <div>
                        <figcaption>Centered around 1</figcaption>
                        <video controls="controls" name="Video Name">
                            <source src="functionapprox/ts_cosine_25.mp4">
                        </video>
                    </div>
                </div>
            </div>
            <br/>
            <br/>
        </div>
        <br/>
    
        <button class="toggle-btn" onclick="toggleSection('linear-regression')"><h2 id="lr" class="center-text">Linear Regression with Polynomial Terms</h2></button>
        <div id="linear-regression" class="toggle-section">
            <p>Taylor Series are effective only when you have the function you're approximating and can take its derivatives. But what if that's not the case? We can still leverage the concept of Taylor Polynomials for function approximation. By using polynomial terms as features in a linear regression model, we can estimate the coefficients, which correspond to the function's derivatives. This approach not only approximates the function but also provides an estimation of its derivatives.</p>


            <div class="image">
                <h3>Approximating Cosine</h3>
                <div class="video-wrapper">
                    <video controls="controls" name="Video Name">
                        <source src="functionapprox/ts_cosine_0.mp4">
                    </video>
                    <div class="table-container">
                        <table>
                            <tr>
                                <th>Polynomial Order</th>
                                <th>Estimated Coefficient</th>
                            </tr>
                            <tr>
                                <td>1</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>2</td>
                                <td>-1.0000</td>
                            </tr>
                            <tr>
                                <td>3</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>4</td>
                                <td>1.0000</td>
                            </tr>
                            <tr>
                                <td>5</td>
                                <td> 0.0000 </td>
                            </tr>
                            <tr>
                                <td>6</td>
                                <td>0.9999</td>
                            </tr>
                            <tr>
                                <td>7</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>8</td>
                                <td>0.9994</td>
                            </tr>
                            <tr>
                                <td>9</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>10</td>
                                <td>-0.9972</td>
                            </tr>
                        </table>
                    </div>
                    <div class="table-container">
                        <table>
                            <tr>
                                <th>Polynomial Order</th>
                                <th>Estimated Coefficient</th>
                            </tr>
                            <tr>
                                <td>11</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>12</td>
                                <td>0.9885</td>
                            </tr>
                            <tr>
                                <td>13</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>14</td>
                                <td>-0.9579</td>
                            </tr>
                            <tr>
                                <td>15</td>
                                <td> 0.0000 </td>
                            </tr>
                            <tr>
                                <td>16</td>
                                <td>0.8670</td>
                            </tr>
                            <tr>
                                <td>17</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>18</td>
                                <td>-0.6536</td>
                            </tr>
                            <tr>
                                <td>19</td>
                                <td>0.0000</td>
                            </tr>
                            <tr>
                                <td>20</td>
                                <td>0.3027</td>
                            </tr>
                        </table>
                    </div>
                </div>
            </div> 

            <div class="video-wrapper" style="padding: 0px 50px">
                <div style="max-width: 800px; padding: 0px 30px">
                    <p>Notice how the polynomial terms, centered around 0, closely match the actual derivatives of the cosine function at \( x = 0 \). When we adjust the polynomial to be centered at \( x = 1 \), the first five Taylor polynomial coefficients reflect the following values:</p>
                    <p>It's important to observe that these coefficients maintain the 4-cycle derivative pattern characteristic of the cosine function.</p>
                    
                </div>
                <div style="padding: 0 60px 0 0">
                    <table>
                        <tr>
                            <th>Polynomial Order</th>
                            <th>Estimated Coefficient</th>
                        </tr>
                        <tr>
                            <td>1</td>
                            <td>-0.8415</td>
                        </tr>
                        <tr>
                            <td>2</td>
                            <td>-0.5403</td>
                        </tr>
                        <tr>
                            <td>3</td>
                            <td>0.8415</td>
                        </tr>
                        <tr>
                            <td>4</td>
                            <td>0.5403</td>
                        </tr>
                        <tr>
                            <td>5</td>
                            <td>-0.8414</td>
                        </tr>
                    </table>
                </div>
            </div>
            <h3 class="center-text">Approximating an Unknown Function</h3>
            <div class="video-wrapper">
                <video controls="controls" name="Video Name">
                    <source src="functionapprox/crazy_func.mov">
                </video>
            </div>
            <p>Here, I created my own function to approximate. The function is \( f(x) = \sqrt{e^{\frac{x}{10}}} \cdot \cos(x) + 4 \)</p>
            <p>The first 3 estimated coefficients for this function are 0.0277, -0.221, and -0.0842. While perhaps a bit tedious, finding the derivative of this function should be something all my former students should have no problem with! The rest of the problem is left to the reader.</p>
            <br/>
        </div>

        <br/>
        <div class="image">
            <button class="toggle-btn" onclick="toggleSection('nn')">
                <h2 id="neural-network" class="center-text">Neural Network (Numpy)</h2>
            </button>
        </div>

        <div id="nn" class="toggle-section">
            <p>Instead of using Taylor polynomials and linear regression for function approximation, I opted for a Fully Connected Neural Network. By coding this neural network from scratch using Numpy, I aimed to deepen my understanding of both the mathematical principles and the vectorization capabilities of Numpy.</p>

            <p>My implementation is highly flexible, allowing users to define both the number of layers and the size of each layer. For hidden layers, I employed the ReLU activation function to introduce non-linearity, while the final layer utilizes a linear activation function. All layers are initialized with HE initialization to ensure optimal weight scaling.</p>
            
            <p>I trained the network using whole-batch gradient descent. Initially, I encountered some training challenges, which I resolved by incorporating momentum and learning rate decay. These enhancements significantly improved the network's performance, making it unnecessary to explore more advanced optimizers like RMSProp or Adam, or additional regularization techniques such as weight decay and dropout. The resulting neural network is robust and efficient, delivering precise function approximations.</p>
            <div class="video-wrapper">
                <div class="image">
                    <figcaption>20 Terms  |  3 Hidden Layers  </figcaption>
                    <figcaption> Learning Rate - 0.01  |  LR Decay - 0.995 (1000 Step Size) </figcaption>
                    <br/>
                    <video controls="controls" width="400" height="400" name="Video Name">
                        <source src="functionapprox/nn_lr01_dc995_steps1000.mp4">
                    </video>
                </div>
                <div class="image">
                    <figcaption> 5 Terms  |  4 Hidden Layers  </figcaption>
                    <figcaption> Learning Rate - 0.01  |  LR Decay - None </figcaption>
                    <br/>
                    <video controls="controls" width="400" height="400" name="Video Name">
                        <source src="functionapprox/nn_lr01_dc1_5terms_4layers.mov">
                    </video>
                </div>
            </div>
            <p>After some further investigation, the difference in performance between the two purely comes down to the extra layer. When given the same number of layers, the models shared remarkably similar results and rates of learning. Turns out this is not very hard for a Neural Network to approximate.</p>
            <p>Notice also that the neural network is able to get extremely precise results with few polynomial terms. Compare the NN's approximation with 10 terms with Taylor Series.</p>
            <div class="video-wrapper">
                <img src="functionapprox/ts_order_10.png" style="max-width: 500px; height: auto;">
                <img src="functionapprox/lr_order10.png" style="max-width: 500px; height: auto;">
            </div>

            <br/>
            <br/>
        </div>

        <button class="toggle-btn" onclick="toggleSection('extend')">
            <h2 id="neural-network" class="center-text">Extensions</h2>
        </button>

        <div id="extend" class="toggle-section" style="padding:10px 30px">
            <h4>Visualizing Local Critical Points</h4>
            <div class="video-wrapper">
                <img src="functionapprox/local_critical_point.png" style="max-width: 500px; height: auto;">
            </div>

            <p>With some instances, I saw the result above. This was a very interesteding diepcation of a common challenge in training neural networks: getting stuck in local optima. In this scenario, the neural network's predictions do not accurately capture the true underlying function, and instead, the model converges to a suboptimal approximation. This phenomenon occurs when the optimization process stalls in a local minimum of the loss function, preventing the network from finding a more accurate global solution.</p>

            <p>With a simple optimizer like gradien descent with momentum, it is easy to get stuck in local optima depending on the initlaization. Initialization plays a crucial role in the training of neural networks and can significantly impact the final model performance. The weights of the network are typically initialized with small random values. The choice of these initial values can influence the trajectory of the optimization process. If the weights are initialized poorly, the network might converge to a local optimum instead of the global optimum, resulting in suboptimal performance.</p>
        </div>
    
        <h2 id="github" class="center-text">Github Repository</h2>

        <p class="center-text"><a href="https://github.com/chenjoshua7/functionApproximation">Github for this project can be found here.</a></p>
    </div>
    
    <div class="btm_banner">
        © Copyright 2024 All rights reserved. |  
        <a href="tos.html" style="color:white">Terms of Service</a>   |  
        <a href="tos.html" style="color:white">Privacy Policy</a>   |  
        <a href="mailto:chen.joshua98@gmail.com", style="color:white">Contact Me</a>
    </div>

</body>
</html>
