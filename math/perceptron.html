<!DOCTYPE html>
<html lang="en">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-W57V1PRPVC"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-W57V1PRPVC');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="../template.css">
    <title>Visualizing the Perceptron Algorithm</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background: repeating-linear-gradient(to bottom, rgb(0, 183, 255), #54400e 100vh, rgb(0, 183, 255) 200vh );
        }
        a {
            color:white;
        }
        .center-text {
            text-align: center;
        }


        .percept {
            color: white;
            padding: 10px 50px 10px 50px;
            margin-top: 10px;
            text-align: justify;
            text-justify: inter-word;
            font-size: 18px;
        }

        .visual {
            color: white;
            text-align: center;
            position: relative;
            display: flex;
            justify-content: center;
            align-items: center;
        }
        .problem {
            margin: 20px;
            
        }
        .problem img {
            width: 50%;
            height: auto;
            border-radius: 10px; 
        }
        .graph {
            text-align: center;
            align-items: center;
            justify-content: center;
            margin: 0 0px;
        }
        
        .graph img {
            border-radius: 10px; 
            width: 80%;
            height: auto;
        }
        .bayes {
            text-align: center;
            align-items: center;
            justify-content: center;
            margin: 0 0px;
        }
        
        .bayes img {
            border-radius: 10px; 
            width: 50%;
            height: auto;
        }

        .visual1 {
            color: white;
            text-align: left;
            position: relative;
            display:flex
        }
        .texts {
            display: block;
            justify-content: center;
            margin: 10px 10px 20px 0px;
            max-width: 600px;
        }
        .result {
            justify-content: center;
            margin: 0 0px;
            align-items: right;
            text-align: right;
        }
        
        .result img {
            border-radius: 10px; 
            width: 100%;
            height: auto;
        }

        @media (max-width: 768px) {
            .banner {
                font-size: 16px;
            }

            .hero-text {
                font-size: 14px;
            }

            .about {
                padding: 20px 5%;
            }
        }

        @media (max-width: 480px) {
            .banner {
                padding: 15px 10px;
            }

            .hero-text {
                font-size: 12px;
            }
        }


    </style>
</head>

<body>
    <div class="banner">
        <a href= "#"> </a>
        <a href= "#"> </a>
        <a href="/index.html">Home</a>
        <a href="/resume.html">Resume</a>
        <div class="dropdown">
            <a href="/math.html"><button class="dropbtn">Math</button></a>
            <div class="dropdown-content">
                <a href="/math/perceptron.html">Perceptron</a>
                <a href="/math/pandemic.html">Pandemic Modeling</a>
                <a href="/math/cod.html">Curse of Dimensionality</a>
            </div>
        </div>
    
        <div class="dropdown">
            <a href="/projects.html"><button class="dropbtn">Projects</button></a>
            <div class="dropdown-content">
                <a href="/projects/recommendation.html">MovieLens Recommendation</a>
                <a href="/projects/nlp.html">BERT Classification</a>
                <a href="/projects/gspmulticlass.html">Urban Land Covers Class-<br/>ification with Linear Models</a>
                <a href="/projects/basketball.html">Lakers 23-24 Analysis</a>
            </div>
        </div>
        <a href="/other.html">Other Stuff</a>
        <a href= "#"> </a>
        <a href= "#"> </a>
    </div>
    
    <div class="backgd">
        <div class="percept">
 
            <h1 class="center-text"> Understanding and Visualizing the Percpetron Algorithm</h1>
            <p> First, if you are interested in viewing my Notebook to play around with or following along, click <a href="https://github.com/chenjoshua7/chenjoshua7.github.io/blob/main/notebooks/Question10.ipynb">here.</a></p>
            <p> 
                As part of my Machine Learning course with Professor Gabor Lugosi, part of our homework was to code the Perceptron Algorithm from scratch and apply it to generated linearly separable and linearly non separable data across different sample sizes, dimensions, and degrees of separability. This was then compared to the Bayes Risk.
            </p>
            <p>
                Coding this was quite a fun challenge and gave me admittedly a fondness for this simple algorithm. For sure, the Perceptron is incredibly limited and I didn't understand at first why we were learning such an ancient algorithm. However, it ended up being a great introduction into empirical risk minimization and helped me thinking about data as projections in space in a new light.
            </p>
            <p>
                Additionally, I added a visualization feature in my algorithm to help me see what the algorithm was doing in two dimensional cases.
            </p>
            <p>
            </p>
        </div>

        <div class="visual">
            <div class="problem">
                <figcaption>The Problem Given to Us</figcaption>
                <br/>
                <img src="../images/q10_ml.png" alt="problem10">        
            </div>
        </div>

        <div class="visual">
            <div class="graph">
                <figcaption>Linearly Separable Data with Alpha = 0.1</figcaption>
                <br/>
                <img src="../perceptron_gifs/alpha05.gif" alt="Perceptron Alpha">
            </div>
            <div class="graph">
                <figcaption>Linearly Inseparable Data with m = 3</figcaption>
                <br/>
                <img src="../perceptron_gifs/linearlyns.gif" alt="Linearly NS">
            </div>
        </div>


        <div class="percept">
            <a id = "generate"></a>
            <h2 class="center-text"> Generating the Data</h2>
            <p>To generate the data for this binary classificaiton problem, I created data from two distributions.</p>
            <p>In the linearly separable case, we pull from uniform distributions and we set the degree of seperation with al</p>
            <p>For the linearly the linearly inseparable case, we draw from multivariate normals. However, for the first dimension, we set the mean to be 0 for one class and <i>m</i> for the other class. The means for the rest of the dimensions are 0.</p>
        </div>

        <div class="percept">
            <a id = "algo"></a>
            <h2 class="center-text"> The Algorithm</h2>
            <p>
                This algorithm was invented on the 50’s and learns a neuron (a linear classifier is basically a neural network with one neuron).
                </p>
                <p>
                Suppose that the data is nearly separable. Let’s see the algorithm.
                </p>
                <ol>
                <li>We start with an arbitrary \( w_0 \), for example \( w_0 = \vec{0} \).</li>
                <li>Then we go through our data points one by one.<br>
                At time \( t \):
                    <ul>
                    <li>If \( w_{t-1} X_t Y_t > 0 \) then we do nothing.</li>
                    <li>If \( w_{t-1} X_t Y_t \leq 0 \) then \( w_t = w_{t-1} + X_t Y_t \).</li>
                    </ul>
                </li>
                <li>Stop when all data points are correctly classified. Cycle through the data several times if needed.<br>
                </li>
                </ol>
            Linearly separable data is a huge assumption. As we see in the visualization, we must create a maximum number of iterations or else it will run forever (as there will always be at least one misclassified point by definition.)
        </div>

        <div class="percept">
            <a id = "results"></a>
            <h2 class="center-text"> Results </h2>
            <p>
                To obtain our results, we tested the perceptron on numerous generated datasets. We tested across different dimensions, sample sizes, and degree of separabilities
                (alpha for separable data and m for nonseparable data as defined above). We tested each combination of parameters 20 times. By using 10,000 generated out of sample
                testing points, we calculate the average risk (1 - accuracy) to estimate the true risk. See the results below: 
            </p>
            <br/>
            <div class="visual1">
                <div class="texts">
                    <h4> Separable Data </h4>
                    <p>
                        On the right, we show the results of the perceptron algorithm on across different sample sizes, dimensions, and degrees of separability (alpha).
                        Greater separability and greater sample sizes will intuitively lower the estimated risk. However, we see the Curse of Dimensionality in effect
                        as greater dimensionality results in greater risk across all sample sizes and degrees of separability. 
                    </p>
                </div>
                <br/>
                <div class="result">
                    <img src="../perceptron_gifs/separable_results.png" alt="Separable Results">
                </div>
            </div>
            <br/>
    </div>

    <div class="percept">
        <a id = "generate"></a>
        <h2 class="center-text"> Bayes Risk</h2>
            <p> The Bayes Classifier is the ideal classifier if the distribution of the data is known. The Bayes Risk is simply the expected risk of the Bayes Classifier.
                FGr linearly separable data, the Bayes Risk is 0. There is no overlap in the distirbutions so we know every time which class the data belongs to given the
                class distributions.</p>
            <p> For our case of linearly inseparable data, below my calculations for the Bayes Risk.</p>
            <div class="bayes">
                <img src="../perceptron_gifs/bayes_proof.png">
            </div>
            <p>
            Using this, we can compare our results above to the Bayes Risk as demonstrated by the black line.</p>
            <div class="graph">
                <img src="../perceptron_gifs/ns_separable_withbayes.png">
            </div>
    </div>
    <div class="percept">
 
        <h1 class="center-text"> Extensions </h1>
        <p>
            If you read through all that... Thank you. Here were a couple thoughts of how you can expand on what I did.</p>
        <br/>
        <h3>Non-Linearly but Separable Data Kernels</h3>
        <p>What if we kernalized the perceptron classificer? This would be helpful for data that is separable but not necessarily linearly separable. 
            Even for non-separable data, the bayes classifier here is linear because the covariance matrices for both classes are the same. But what if they weren't?
        </p>
        <h3>Support Vector Machines</h3>
        <p>All the perceptron cares about is separating the data. SVM works different by trying to maximize the margin. Try doing the same thing with hard-margin SVM
            and the try it against with soft-margin SVM. Let me knwo if you do?
        </p>
        <h3>Visualziations Expansions</h3>
        <p>
        Visualize perceptron in 3 dimensions. Then do it in 4 dimensions (just kidding.) Also, an improvment on my visualization would be, for each update, the
        misclassified point in question is highlighted.
        </p>

        <br/>
        <p class="center-text">If you do any of these, please let me know. I'd love to see! Thanks for reading!</p>
    </div>
    </div>
    <div class=btm_banner>
        © Copyright 2024 All rights reserved. Designed and developed from scratch by Joshua Chen.
    </div>
</body>
</html>